# ðŸ“š RAG Assistant (Streamlit)

A Retrieval-Augmented Generation (RAG) application built with **Streamlit**, **LangChain 1.x**, **ChromaDB**, and **Groq**.

Upload documents and ask questions â€” answers are generated **only from your data**, not from the modelâ€™s imagination.

---

## ðŸš€ Features

- âœ… Multi-file upload
- âœ… Supports **TXT** and **PDF**
- âœ… Vector search using **ChromaDB**
- âœ… Fast inference with **Groq**
- âœ… No hallucinations (context-grounded answers)
- âœ… Clean Streamlit UI

---

## ðŸ§  How It Works

1. Documents are uploaded (TXT / PDF)
2. Text is split into chunks
3. Chunks are embedded using `sentence-transformers`
4. Stored in ChromaDB
5. Relevant chunks are retrieved per question
6. Groq LLM generates an answer using **only retrieved context**

---

## ðŸ›  Tech Stack

- **Frontend:** Streamlit
- **LLM:** Groq (llama-3.1-8b-instant)
- **Embeddings:** HuggingFace (`all-MiniLM-L6-v2`)
- **Vector Store:** ChromaDB
- **Framework:** LangChain 1.x

---

## ðŸ“¦ Installation (Local)

```bash
git clone https://github.com/TahaHasansk/rag-assistant-streamlit.git
cd rag-assistant-streamlit
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
